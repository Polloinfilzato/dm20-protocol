---
name: Prefetch Engine
status: closed
updated: 2026-02-20T00:48:43Z
created: 2026-02-19T23:14:33Z
updated: 2026-02-19T23:18:18Z
github: https://github.com/Polloinfilzato/dm20-protocol/issues/172
depends_on: []
parallel: true
conflicts_with: []
---

# Task: Prefetch Engine

## Description

Build an intelligent prefetch system that anticipates likely DM responses by monitoring game state context. In combat, pre-generate 2-3 narrative variants (hit/miss/critical) using the campaign's main model, then use Haiku to select and refine the correct variant when the actual result is known. This reduces response latency by 50%+.

## Acceptance Criteria

- [ ] Context observer monitors game state (combat, exploration, dialogue)
- [ ] Combat start preloads relevant stat blocks, PC spell lists, action options
- [ ] Player turn triggers pre-generation of 2-3 narrative variants
- [ ] Main model (per campaign profile) generates variants
- [ ] Haiku model selects correct variant and refines with actual values
- [ ] Actual response latency reduced by 50%+ compared to non-prefetch
- [ ] Prefetch activates automatically in combat (highest value context)
- [ ] Cache invalidation when game state changes unexpectedly
- [ ] Prefetch does not interfere with normal request processing
- [ ] Token cost of prefetch tracked and reported in session summary
- [ ] Prefetch intensity configurable: off, conservative, aggressive
- [ ] Cache has TTL-based expiration for stale variants

## Technical Details

### New Package: `src/dm20_protocol/prefetch/`

#### `observer.py` — Game State Context Observer

```python
class ContextObserver:
    """Monitors game state changes and triggers prefetch."""
    def on_state_change(self, game_state):
        context = self._classify_context(game_state)
        if context == "combat":
            self.trigger_combat_prefetch(game_state)
```

#### `cache.py` — Prefetch Variant Cache

```python
class PrefetchCache:
    """TTL-based cache for pre-generated narrative variants."""
    def store(self, key: str, variants: list, ttl: int = 60): ...
    def get(self, key: str) -> list | None: ...
    def invalidate(self, pattern: str): ...
```

#### `engine.py` — Pre-generation + Refinement Pipeline

```python
class PrefetchEngine:
    async def pre_generate_combat_variants(self, game_state, player_turn):
        prompts = self._build_variant_prompts(game_state, player_turn)
        variants = await self.main_model.generate_batch(prompts)
        self.cache.store(player_turn.id, variants)

    async def resolve_with_actual(self, turn_id, actual_result):
        variants = self.cache.get(turn_id)
        if not variants:
            return await self.main_model.generate(actual_result)
        refined = await self.haiku_model.select_and_refine(
            variants=variants, actual=actual_result
        )
        return refined
```

### Token Usage Tracking

- Track tokens used for pre-generation (variant generation cost)
- Track tokens saved (avoided full generation)
- Report in session summary: "Prefetch: X tokens used, Y tokens saved, Z% cache hits"

### Files Affected

- `src/dm20_protocol/prefetch/` — New package (observer.py, cache.py, engine.py, __init__.py)
- `tests/test_prefetch/` — Unit and integration tests

## Dependencies

- [ ] No strict task dependencies
- [ ] Requires Anthropic API access for Haiku model tier
- [ ] Requires existing game state and combat infrastructure

## Effort Estimate

- Size: L
- Hours: 12-16
- Parallel: true (independent of all other tasks)

## Definition of Done

- [ ] Code implemented
- [ ] Unit tests for cache, observer, engine
- [ ] Integration tests for combat prefetch pipeline
- [ ] Token usage tracking verified
- [ ] Latency reduction measured and documented
- [ ] CHANGELOG.md updated
